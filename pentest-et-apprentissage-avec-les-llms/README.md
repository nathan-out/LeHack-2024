# Améliorer l'apprentissage et le pentest grâce aux LLMs

*Jérémie Amsellem*

- [LP1EU - Twitter](https://twitter.com/lp1eu)

- [LP1EU - LinkedIn](https://www.linkedin.com/in/lp1eu/)

- [Hackers.com - LP1](https://hackers.town/@lp1)

Retour d'expérience d'un pentesteur qui a implémenté un LLM pour ses étudiants et ses missions de pentest.

## Entraînement au social engineering

L'objectif était de créer un modèle qui prendrait le rôle d'un employé lambda. L'objectif est d'essayer de lui soutirer des informations par le biais de social engineering ou du phishing. Il faut pouvoir générer des sessions d'entraînement rapidement, à partir de plusieurs scénarios et que le modèle soit rapide à répondre au auto-hébergé.

Les LLMs possèdent plusieurs paramètres mais deux sont importants ici : la température et la taille du contexte. Ils correspondent respectivement à la créativité du modèle (la liberté qu'il prend dans sa génération de texte), et la mémoire immédiate du LLM. Plus cette dernière est importante, plus on peut donner de contexte au modèle et plus il se rappellera des messages précédents. Cependant, cela impacte la vitesse d'inférence (génération de texte).

Pour configurer le modèle, on peut lui donner un input système qui va servir à contextualiser sa tâche. Cependant, lorsqu'on parle de cybersécurité, les modèles sont souvent frilleux car censurés.

Le modèle LLAMA3 était censuré (il existe aujourd'hui des versions non censurées) et une inférence prend environ 30 secondes en local.

Phi-3 obtient de bons résultats (10 secondes de générations) et est moins censuré.

## Simuler des employés

On utilise plusieurs bots qui correspondent à plusieurs employés de la société. Chaque bot possède des informations différentes, d'un niveau de confidentialité divers, de leviers plus ou moins importants...

Résultat, plusieurs LLM ont halluciné. C'est-à-dire qu'ils inventent complètement un nouveau contexte et sortent du scénario pré-établi. C'est particulièrement vrai pour Phi-3 et LLAMA3. Les réponses sont clairement décevantes.

Pour le modèle de Mistral, les instructions ne sont pas bien suivies non plus mais il n'y a pas eu d'hallucination observées.

Point intéressant, WizardLM2, qui est un modèle conçu pour répondre à des inputs complexes produit des résultats plutôt bons. Il a peu d'hallucination mais les temps de générations sont plus longs.

## Quizz intéractif

Car les QCM sont particulièrement peu funs.

Phi-3 n'est pas concluant, à moins de passer par un script Python.

## Aider les pentesteurs

L'auteur a utilisé un RAG qui semble être de la même nature que le *fine-tuning* mais moins gourmand. Le modèle doit être 100% confidentiel et ne pas casser le système testé.

Il existe un outil qui s'appelle Pentest GPT qui est plutôt bon mais qui est très guidé. Cela s'explique par le fait qu'il a été programmé par des spécialistes du domaine pour cette tâche précise.
